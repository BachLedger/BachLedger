{
  "stories": [
    {
      "title": "Genesis Block Creation Flow",
      "tags": ["call-chain", "genesis", "blockchain"],
      "content": "When initializing a new blockchain, CreateGenesis in genesis.go is called with ChainConfig. It generates a config transaction via genConfigTx() containing chain config as payload, then generates the read-write set via genConfigTxRWSet() which initializes all system contracts. The function calculates hashes: RwSetHash for the transaction, TxHash, then TxRoot (merkle root), RwSetRoot, DagHash, and finally BlockHash. The genesis block has height 0, type CONFIG_BLOCK, and a fixed default timestamp. System contracts are initialized based on block version - newer versions (>=2.3.0) also create address-based contract entries."
    },
    {
      "title": "Transaction Verification Flow",
      "tags": ["call-chain", "transaction", "security"],
      "content": "VerifyTxWithoutPayload verifies a transaction in three steps: (1) verifyTxHeader checks non-nil header, correct chain ID, TxId length/format, timestamp before expiration; (2) verifyTxAuth builds authentication principal from sender endorsement and tx bytes, looks up exceptional policies, then calls AccessControlProvider.VerifyPrincipal; (3) for INVOKE_CONTRACT type with endorsers, additional policy verification is performed including special handling for SELF rule which requires org_id parameter. Payer endorsement is also verified if present."
    },
    {
      "title": "Block Hash Calculation Flow",
      "tags": ["call-chain", "block", "crypto"],
      "content": "CalcBlockHash computes block hash by: (1) copying block header, (2) setting Signature and BlockHash fields to nil, (3) marshaling the header to bytes via protobuf, (4) computing hash using the specified hash type (SHA256, SM3, etc.). This ensures the hash covers all header fields except the signature and hash itself, which are added after signing."
    },
    {
      "title": "WASM Gas Calculation for Syscalls",
      "tags": ["call-chain", "gas", "wasm"],
      "content": "When a WASM contract makes a syscall (e.g., PutState), the gas/wasm module calculates cost based on block version. For version >= 2.3.2: (1) SubtractGasForPutState is called, (2) it retrieves gas prices from wasmDataGasPrice232 map for params/returns, (3) calculateGasForWasmerSyscall232 gets defaultGas from chain config, syscall-specific gas from wasmSyscallGas232 map, (4) common.CalculateGas combines: defaultGas + syscallGas + (paramLen * paramPrice) + (returnLen * returnPrice), (5) txSimContext.SubtractGas deducts from remaining gas. If gas underflows, execution fails."
    },
    {
      "title": "Address Generation from Different Sources",
      "tags": ["call-chain", "address", "identity"],
      "content": "The address module generates addresses from three sources: (1) From public key via PkToAddrStr - for CHAINMAKER type uses SKI (Subject Key Identifier) hash, for ETHEREUM/ZXL uses public key bytes with Keccak256; (2) From certificate via CertToAddrStr - uses SubjectKeyId for CHAINMAKER, marshaled public key for others; (3) From name via NameToAddrStr - applies anti-collision (Keccak256 or SM3 hash of name for version >= 2300) then generates address. All methods call generateAddrStr which applies Keccak256 and takes last 20 bytes for standard addresses, or uses ZXAddress for ZXL type."
    },
    {
      "title": "Blacklist Transaction Filtering",
      "tags": ["call-chain", "security", "filtering"],
      "content": "FilterBlockBlacklistTxs filters transactions by: (1) creating CacheList with chain-specific name ('vm-native' + chainId), (2) iterating through transactions, (3) for each blacklisted TxId (checked via bl.Exists), creating a sanitized transaction copy with contractName and method set to Violation message, parameters cleared, and result message set to Violation, (4) non-blacklisted transactions pass through unchanged. Similar filtering exists for TxRWSet and ContractEvents."
    },
    {
      "title": "Native System Contract Invocation Flow",
      "tags": ["call-chain", "vm-native", "system-contract"],
      "content": "When a system contract is invoked: (1) VmManager routes to vm-native RuntimeInstance based on contract name matching syscontract.SystemContract_*, (2) RuntimeInstance.Invoke determines block version and routes to versioned contract implementation (v210 for version <220, v220 for <2300, current for >=2300), (3) getContractFunc looks up contract by name (with version suffix if needed), then gets method by name from contract.methods map, (4) for version 210-220, additional VERIFY_CONTRACT_ACCESS check is performed, (5) ContractFunc is called with TxSimContext and parameters, (6) function accesses state via TxSimContext.Get/Put/Del, may emit events, returns ContractResult with code/message/result. Gas is deducted from the caller's account if gas accounting is enabled."
    },
    {
      "title": "TBFT Block Signature Verification Flow",
      "tags": ["call-chain", "consensus", "security"],
      "content": "VerifyBlockSignatures verifies TBFT consensus signatures: (1) extracts VoteSet from block.AdditionalData.ExtraData[TBFTAddtionalDataKey], (2) unmarshals VoteSet protobuf, (3) retrieves chain config at block height via chainConf.GetChainConfigFromFuture, (4) gets validator list from config using validatorListFunc, (5) creates validatorSet with validators and default blocksPerProposer, (6) creates VoteSet from proto and checks twoThirdsMajority - requires >2/3 validators to vote for same hash, (7) verifies VoteSet hash matches block hash, (8) calls verifyVotes which spawns goroutines to verify each vote signature in parallel: clones vote, sets endorsement to nil, marshals for signature verification, creates principal via AccessControlProvider, calls VerifyPrincipal to validate signature. Returns error if any vote fails verification or quorum not reached."
    },
    {
      "title": "TBFT Baseline Configuration Test (4 Nodes)",
      "tags": ["test-scenario", "consensus", "baseline"],
      "content": "Standard baseline test for TBFT consensus with minimum Byzantine fault tolerance: (1) Four-node TBFT network (yz-org1 through yz-org4) running on Docker with IPs 192.168.2.2-5, (2) Provides f=1 Byzantine fault tolerance (n=4 satisfies 3f+1=4), meaning network tolerates 1 Byzantine failure, (3) Requires 3+ nodes (>2/3 of 4) for quorum on proposals and commits, (4) Uses ephemeral storage (no persistent volumes) for quick iteration during development and functional testing, (5) Each node mounts all 4 organization certificates for full mesh trust configuration, (6) Includes debugging capabilities: GODEBUG=madvdontneed=1 environment variable and SYS_PTRACE capability for profiling, (7) Uses explicit config path via '-c ../config/yz-org{N}/yzchain.yml' command. This is the reference implementation and starting point for TBFT testing - the simplest production-viable BFT configuration. All other test scenarios (persistence, multi-chain, dynamic membership) build upon this baseline. Suitable for quick functional validation, contract testing, and development workflows where state persistence isn't required. Test located in chain-module/consensus-tbft/testdata/four_node/."
    },
    {
      "title": "Dynamic Organization Addition in TBFT Network",
      "tags": ["test-scenario", "consensus", "governance"],
      "content": "Test scenario for adding a 5th organization to a running 4-node TBFT network: (1) Start nodes 1-4 representing yz-org1 through yz-org4 in a Docker network (192.168.2.2-5), (2) Deploy and invoke a test contract to verify network operation, (3) Use send_proposal_request_tool to add org5's trust root certificate via trustRootAdd proposal - requires majority approval from existing 4 organizations, (4) Use chainConfigNodeOrgAdd proposal to add org5's consensus node address (/ip4/127.0.0.1/tcp/11305/p2p/QmVSCXfPweL1GRSNt8gjcw1YQ2VcCirAtTdLKGkgGKsHqi) to chain config, (5) Start node5 which syncs from existing network and joins consensus. This tests TBFT's dynamic membership capability where validators can be added without stopping the network. Test located in chain-module/consensus-tbft/testdata/four_node_add_one_org/."
    },
    {
      "title": "TBFT Crash Recovery and Persistence Test",
      "tags": ["test-scenario", "consensus", "reliability"],
      "content": "Stress test for TBFT's crash recovery and state persistence: (1) Four-node TBFT network (yz-org1 through yz-org4) runs with persistent Docker volumes mounted for ledgerData and logs, (2) Test script repeatedly executes 'docker-compose down' followed by 'docker-compose up -d' in a loop (10,000 iterations), (3) Random sleep interval (0-99 seconds) between each restart simulates unpredictable crash timing, (4) Nodes must recover from WAL on each restart, rejoin consensus, and continue producing blocks without data loss or fork. This validates that TBFT's WAL-based crash recovery mechanism works correctly under continuous restart stress, ensuring Byzantine fault tolerance survives process crashes. Persistent volumes at ./volume/yz-org{N}/ledgerData preserve blockchain state across container lifecycles. Test located in chain-module/consensus-tbft/testdata/four_node_persistent/."
    },
    {
      "title": "TBFT Large Validator Set Test (10 Nodes)",
      "tags": ["test-scenario", "consensus", "scalability"],
      "content": "Scalability test for TBFT consensus with a larger validator set: (1) Ten-node TBFT network with all 10 organizations (yz-org1 through yz-org10) configured as validators in the consensus nodes list, (2) Network topology uses Docker with IPs 192.168.10.2 through 192.168.10.11, (3) Each node runs with shared crypto-config volume for certificate management, (4) All 10 validators participate in consensus rounds requiring 7+ nodes (>2/3 of 10) for quorum on proposals and commits. This tests TBFT's ability to scale beyond the minimum 4-node setup, validating that the consensus algorithm maintains liveness and safety with increased message complexity (more proposals, prevotes, precommits to broadcast and verify). Tests proposer rotation across 10 validators and ensures network can tolerate up to 3 Byzantine failures (f=3 for n=10, where n >= 3f+1). Test located in chain-module/consensus-tbft/testdata/ten_node/."
    },
    {
      "title": "Multi-Chain TBFT with Persistence Test",
      "tags": ["test-scenario", "consensus", "multi-chain", "reliability"],
      "content": "Test for running multiple independent blockchain chains simultaneously on same physical nodes with TBFT consensus: (1) Four-node network (yz-org1 through yz-org4) with each node configured to run 3 separate chains (chain1, chain2, chain3) in yzchain.yml blockchain section, (2) Each chain has independent genesis config: chain1→bc1.yml, chain2→bc2.yml, chain3→bc3.yml, (3) Persistent volumes mounted at /tmp/yzchain/{org}/ledgerData and /tmp/yzchain/{org}/log preserve state for all chains across restarts, (4) Each chain runs independent TBFT consensus with same 4 validators but separate state machines, block heights, and consensus rounds. This validates that a single ChainMaker node can participate in multiple blockchain networks concurrently, with each chain maintaining independent Byzantine fault tolerance. Each chain requires separate WAL and storage but shares network layer and node identity. Test located in chain-module/consensus-tbft/testdata/four_node_three_chain_persistent/."
    },
    {
      "title": "TBFT Single Node Development Test (1 Node)",
      "tags": ["test-scenario", "consensus", "development", "edge-case"],
      "content": "Extreme edge case test for TBFT consensus with single node only: (1) One-node TBFT network with only yz-org1 as both the sole validator and sole trust root (192.168.2.2), (2) Provides absolutely NO fault tolerance or Byzantine resistance (f=0, n=1) - essentially running TBFT consensus engine in pseudo-solo mode, (3) Any failure of the single node halts the entire network permanently, (4) No consensus voting occurs - the single node trivially achieves >2/3 quorum with itself (1/1 = 100%), making all proposals instantly final, (5) Still mounts certificates for 4 organizations despite only using org1, possibly for configuration compatibility or future expansion testing. This configuration is purely for rapid development iteration, TBFT engine smoke testing, or educational purposes to understand consensus basics without network complexity. NOT suitable for any production use or even serious testing - defeats the entire purpose of Byzantine fault tolerance. ChainMaker also provides proper SOLO consensus type (type=0) for true single-node scenarios. Test located in chain-module/consensus-tbft/testdata/one_node/."
    },
    {
      "title": "TBFT Minimum Configuration Test (2 Nodes)",
      "tags": ["test-scenario", "consensus", "edge-case"],
      "content": "Edge case test for TBFT consensus with minimal node configuration: (1) Two-node TBFT network with only yz-org1 and yz-org2 as validators (192.168.2.2-3), (2) Consensus requires >2/3 quorum, meaning both nodes must agree (2/2 = 100% > 66.7%), (3) Network can produce blocks and reach consensus as long as both nodes are operational and honest, (4) Cannot tolerate ANY node failures or Byzantine behavior - if one node crashes or acts maliciously, consensus halts completely (f=0 for n=2, where n=3f+1 requires n≥4 for f≥1). This tests TBFT's behavior in sub-optimal configurations suitable for development/testing environments where full Byzantine fault tolerance isn't required. Validates that TBFT can operate with minimal infrastructure while understanding it provides only crash fault tolerance (like CFT) rather than full BFT guarantees. Test located in chain-module/consensus-tbft/testdata/two_node/."
    },
    {
      "title": "TBFT Production Configuration Test (7 Nodes)",
      "tags": ["test-scenario", "consensus", "production", "performance"],
      "content": "Practical production configuration test for TBFT consensus with optimal Byzantine fault tolerance: (1) Seven-node TBFT network (yz-org1 through yz-org7) running on Docker with IPs 192.168.2.2-8, (2) Provides f=2 Byzantine fault tolerance (n=7 satisfies 3f+1=7), meaning network tolerates up to 2 simultaneous Byzantine failures while maintaining consensus, (3) Requires 5+ nodes (>2/3 of 7) for quorum on proposals and commits - can continue operating even if 2 nodes crash or behave maliciously, (4) Persistent volumes at /tmp/yzchain/{org}/ledgerData and /tmp/yzchain/{org}/log for state preservation across restarts, (5) Includes csv.sh utility script for extracting memory profiling data (mem.csv) from logs for performance analysis. This represents recommended production deployment size - better fault tolerance than minimum 4-node setup (f=1) while more efficient than large 10-node setup (f=3). Command output redirected to /yzchain/log/panic.log for crash debugging. Test located in chain-module/consensus-tbft/testdata/seven_node/."
    },
    {
      "title": "TBFT Kubernetes Deployment Test (4 Nodes)",
      "tags": ["test-scenario", "consensus", "kubernetes", "cloud-native"],
      "content": "Cloud-native Kubernetes deployment for TBFT consensus network: (1) Four-node TBFT network (yz-org1 through yz-org4) deployed as Kubernetes Deployments with replica count of 1 each, (2) Uses Kubernetes ClusterIP Services for P2P communication with fixed IPs (10.96.1.1 through 10.96.1.4) enabling stable network addressing, (3) NodePort Services expose grpc (7988), monitor (17989), and pprof (4321) ports externally via ports 301XX-304XX range for external access and debugging, (4) Configuration and crypto materials distributed via ConfigMap - main.sh script packages both into crypto.tgz and config.tgz, then creates ConfigMap that pods mount at /tmp, (5) Each pod extracts configs at startup before launching yzchain with '-e yz-org{N}' flag, (6) Node affinity (nodeSelector) pins each pod to specific physical hosts (vm-16-17-centos, vm-16-5-centos, vm-16-6-centos, vm-16-8-centos) for predictable deployment, (7) Pulls container images from private registry at 172.21.16.17:5000/yzchain with Always pull policy. This demonstrates ChainMaker's cloud-native deployment capabilities with proper Kubernetes networking, service discovery, and orchestration. The main.sh script provides simple start/stop commands that handle ConfigMap creation and deployment lifecycle. Test located in chain-module/consensus-tbft/testdata/four_node_k8s/."
    },
    {
      "title": "TBFT Production Multi-Chain Test (7 Nodes, 3 Chains)",
      "tags": ["test-scenario", "consensus", "production", "multi-chain", "performance"],
      "content": "Combined production-ready and multi-chain test for TBFT consensus at scale: (1) Seven-node TBFT network (yz-org1 through yz-org7) with f=2 Byzantine fault tolerance running 3 independent blockchain chains simultaneously, (2) Each node configured to run chain1, chain2, and chain3 via yzchain.yml blockchain section with separate genesis configs (bc1.yml, bc2.yml, bc3.yml), (3) All 7 validators participate in consensus for each of the 3 chains independently - requires 5+ nodes (>2/3 of 7) for quorum on each chain, (4) Each chain maintains separate state machines, block heights, consensus rounds, and WAL while sharing network layer and node infrastructure, (5) Persistent volumes at /tmp/yzchain/{org}/ledgerData and /tmp/yzchain/{org}/log preserve state for all chains across restarts, (6) Includes csv.sh utility for memory profiling across all chains and nodes, (7) Network topology uses Docker with IPs 192.168.2.2-8 and command output redirected to /yzchain/log/panic.log. This tests ChainMaker's ability to run production-grade multi-chain infrastructure where a single physical network provides multiple blockchain services with independent Byzantine fault tolerance for each chain. Validates that optimal BFT configuration (f=2) scales across multiple concurrent chains without compromising safety or liveness. Test located in chain-module/consensus-tbft/testdata/seven_node_three_chain/."
    }
  ]
}
